[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "vllm-mlx"
version = "0.2.0"
description = "Apple Silicon MLX backend for vLLM - GPU-accelerated LLM inference on Mac with continuous batching"
readme = "README.md"
license = {text = "Apache-2.0"}
requires-python = ">=3.10"
authors = [
    {name = "vllm-mlx contributors"}
]
keywords = ["llm", "mlx", "apple-silicon", "vllm", "inference", "transformers"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: MacOS",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

dependencies = [
    "mlx>=0.29.0",
    "mlx-lm>=0.20.0",
    "mlx-vlm>=0.1.0",  # VLM support
    "transformers>=4.40.0",
    "tokenizers>=0.19.0",
    "huggingface-hub>=0.23.0",
    "numpy>=1.24.0",
    "pillow>=10.0.0",
    "tqdm>=4.66.0",
    "pyyaml>=6.0",
    "gradio>=4.0.0",
    "requests>=2.28.0",
    "tabulate>=0.9.0",
    # Video processing for VLM
    "opencv-python>=4.8.0",
    # Resource monitoring
    "psutil>=5.9.0",
    # Server
    "fastapi>=0.100.0",
    "uvicorn>=0.23.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
]
vllm = [
    "vllm>=0.4.0",
]

[project.urls]
Homepage = "https://github.com/vllm-mlx/vllm-mlx"
Documentation = "https://github.com/vllm-mlx/vllm-mlx#readme"
Repository = "https://github.com/vllm-mlx/vllm-mlx"

[project.entry-points."vllm.platform_plugins"]
mlx = "vllm_mlx.plugin:mlx_platform_plugin"

[project.scripts]
vllm-mlx = "vllm_mlx.cli:main"
vllm-mlx-serve = "vllm_mlx.server_v2:main"
vllm-mlx-chat = "vllm_mlx.gradio_app:main"
vllm-mlx-bench = "vllm_mlx.benchmark:main"

[tool.setuptools.packages.find]
where = ["."]
include = ["vllm_mlx*"]

[tool.black]
line-length = 88
target-version = ["py310", "py311", "py312", "py313"]

[tool.ruff]
line-length = 88
select = ["E", "F", "W", "I", "N", "UP", "B", "SIM"]
ignore = ["E501", "B905"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
asyncio_mode = "auto"
